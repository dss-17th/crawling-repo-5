{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "% % time\n",
    "\n",
    "\n",
    "def crawlingquery(symbol):\n",
    "    url = f\"https://investing.com/search/?q={symbol}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # 맞는 데이터 찾기\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    datas = dom.find_all(class_=\"js-inner-all-results-quote-item row\")\n",
    "    investing_query = np.nan\n",
    "\n",
    "    for i in range(len(datas)):\n",
    "        flag = datas[i].find(class_=\"flag first\")\n",
    "        try:\n",
    "            if re.findall('middle (USA)', str(flag))[0] != 'USA':\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        investingsymbol = datas[i].find(class_=\"second\").text\n",
    "        if investingsymbol.upper() != symbol.upper():\n",
    "            continue\n",
    "\n",
    "        text = datas[i].find(class_=\"fourth\").text\n",
    "        if text[:5] != 'Stock':\n",
    "            continue\n",
    "\n",
    "        investing_query = re.findall(\n",
    "            'equities\\/([\\S]+)', datas[i].get(\"href\"))[0]\n",
    "        return investing_query\n",
    "\n",
    "\n",
    "def crawling_investing(symbol, query):\n",
    "    try:\n",
    "        url = f\"https://kr.investing.com/equities/{query}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "        elements = dom.find_all(\n",
    "            class_='flex justify-between border-b py-2 desktop:py-0.5')\n",
    "        # time.sleep(0.5)\n",
    "        for element in elements:\n",
    "            try:\n",
    "                str = re.findall('발행주식수([0-9\\,]+)', element.text)[0]\n",
    "                shared = int(str.replace(',', ''))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                sales = re.findall('매출([0-9\\.A-Z]+)', element.text)[0]\n",
    "                sales = float(sales[:-1]) * int(sales[-1].replace('B',\n",
    "                                                                  str(10000*10000*10)), replace('M', str(10000*100)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                str = re.findall('다음 수익일자([0-9년월일\\s]+)', element.text)[0]\n",
    "                year = int(re.findall('(2[12])년', str)[0])\n",
    "                month = int(re.findall('([0-9]{1,2})월', str)[0])\n",
    "                date = int(re.findall('([0-9]{1,2})일', str)[0])\n",
    "                update_date = year*10000 + month*100 + date + 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return [symbol, sales, shared, update_date]\n",
    "\n",
    "    except:\n",
    "        return [symbol, np.nan, np.nan, np.nan]\n",
    "\n",
    "\n",
    "def crawling_investing2(symbol, query):\n",
    "    first_date = first_sales = first_profit = first_operating = first_realprofit = second_date = second_sales = second_profit = second_operating = second_realprofit = thirth_date = thirth_sales = thirth_profit = thirth_operating = thirth_realprofit = fourth_date = fourth_sales = fourth_profit = fourth_operating = fourth_realprofit = np.nan\n",
    "    try:\n",
    "        url = f\"https://kr.investing.com/equities/{query}-financial-summary\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "        dates = dom.select(\"#rsdiv > div > table > thead > tr > th\")\n",
    "        numbers = dom.select(\"#rsdiv > div > table > tbody > tr > td\")\n",
    "        # time.sleep(0.5)\n",
    "\n",
    "        for date, i in zip(dates, range(len(dates))):\n",
    "            if date.text == '기말:':\n",
    "                str = re.findall('[0-9년월일\\s]+', dates[i+1].text)[0]\n",
    "                year = int(re.findall('(2[0-9])년', str)[0])\n",
    "                month = int(re.findall('([0-9]{1,2})월', str)[0])\n",
    "                day = int(re.findall('([0-9]{1,2})일', str)[0])\n",
    "                first_date = year*10000 + month*100 + day\n",
    "                str = re.findall('[0-9년월일\\s]+', dates[i+2].text)[0]\n",
    "                year = int(re.findall('(2[0-9])년', str)[0])\n",
    "                month = int(re.findall('([0-9]{1,2})월', str)[0])\n",
    "                day = int(re.findall('([0-9]{1,2})일', str)[0])\n",
    "                second_date = year*10000 + month*100 + day\n",
    "                str = re.findall('[0-9년월일\\s]+', dates[i+3].text)[0]\n",
    "                year = int(re.findall('(2[0-9])년', str)[0])\n",
    "                month = int(re.findall('([0-9]{1,2})월', str)[0])\n",
    "                day = int(re.findall('([0-9]{1,2})일', str)[0])\n",
    "                thirth_date = year*10000 + month*100 + day\n",
    "                str = re.findall('[0-9년월일\\s]+', dates[i+4].text)[0]\n",
    "                year = int(re.findall('(2[0-9])년', str)[0])\n",
    "                month = int(re.findall('([0-9]{1,2})월', str)[0])\n",
    "                day = int(re.findall('([0-9]{1,2})일', str)[0])\n",
    "                fourth_date = year*10000 + month*100 + day\n",
    "                break\n",
    "\n",
    "        for number, i in zip(numbers, range(len(numbers))):\n",
    "            if number.text == '총매출':\n",
    "                try:\n",
    "                    first_sales = float(numbers[i+1].text)\n",
    "                    second_sales = float(numbers[i+2].text)\n",
    "                    thirth_sales = float(numbers[i+3].text)\n",
    "                    fourth_sales = float(numbers[i+4].text)\n",
    "                except:\n",
    "                    first_sales = second_sales = thirth_sales = fourth_sales = np.nan\n",
    "            elif number.text == '총 이익':\n",
    "                try:\n",
    "                    first_profit = float(numbers[i+1].text)\n",
    "                    second_profit = float(numbers[i+2].text)\n",
    "                    thirth_profit = float(numbers[i+3].text)\n",
    "                    fourth_profit = float(numbers[i+4].text)\n",
    "                except:\n",
    "                    first_profit = second_profit = thirth_profit = fourth_profit = np.nan\n",
    "\n",
    "            elif number.text == '영업 이익':\n",
    "                try:\n",
    "                    first_operating = float(numbers[i+1].text)\n",
    "                    second_operating = float(numbers[i+2].text)\n",
    "                    thirth_operating = float(numbers[i+3].text)\n",
    "                    fourth_operating = float(numbers[i+4].text)\n",
    "                except:\n",
    "                    first_operating = second_operating = thirth_operating = fourth_operating = np.nan\n",
    "\n",
    "            elif number.text == '순이익':\n",
    "                try:\n",
    "                    first_realprofit = float(numbers[i+1].text)\n",
    "                    second_realprofit = float(numbers[i+2].text)\n",
    "                    thirth_realprofit = float(numbers[i+3].text)\n",
    "                    fourth_realprofit = float(numbers[i+4].text)\n",
    "                except:\n",
    "                    first_realprofit = second_realprofit = thirth_realprofit = fourth_realprofit = np.nan\n",
    "\n",
    "        dfs1 = [symbol, first_date, first_sales,\n",
    "                first_profit, first_operating, first_realprofit]\n",
    "        dfs2 = [symbol, second_date, second_sales,\n",
    "                second_profit, second_operating, second_realprofit]\n",
    "        dfs3 = [symbol, thirth_date, thirth_sales,\n",
    "                thirth_profit, thirth_operating, thirth_realprofit]\n",
    "        dfs4 = [symbol, fourth_date, fourth_sales,\n",
    "                fourth_profit, fourth_operating, fourth_realprofit]\n",
    "\n",
    "    except:\n",
    "        dfs1 = [symbol, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "        dfs2 = [symbol, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "        dfs3 = [symbol, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "        dfs4 = [symbol, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    return dfs1  # , dfs2, dfs3, dfs4\n",
    "\n",
    "\n",
    "querys = pd.read_csv('../datas/investing_query.csv')\n",
    "\n",
    "investing = pd.read_csv\n",
    "today = datetime.now().year % 100 * 10000 + \\\n",
    "    datetime.now().month * 100 + datetime.now().day\n",
    "df = []\n",
    "crawling_investings = []\n",
    "crawling_investings2 = []\n",
    "a = 0\n",
    "for row in querys.values:\n",
    "    symbol = row[0]\n",
    "    query = row[1]\n",
    "    update_date = row[2]\n",
    "    url = f\"https://kr.investing.com/equities/{query}-news\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code >= 300:\n",
    "        a = 1\n",
    "        query = crawlingquery(symbol)\n",
    "        querys['investing_query'][querys['Symbol'] == symbol] = query\n",
    "        url = f\"https://kr.investing.com/equities/{query}-news\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code >= 300:\n",
    "            print(f'{symbol}의 서버가 응답하지 않습니다. 에러 코드 {response.status_code}')\n",
    "\n",
    "    dom = BeautifulSoup(response.content, \"html.parser\")\n",
    "    datas = dom.find_all(class_=\"textDiv\")\n",
    "\n",
    "    for i in range(len(datas)):\n",
    "        item = datas[i]\n",
    "        try:\n",
    "            text = item.select_one('.date').text\n",
    "            date = re.findall('[0-9]{0,2} 시간 전', text)\n",
    "        except:\n",
    "            continue\n",
    "        title = item.select_one(\".title\").get(\"title\")\n",
    "        link = 'https://kr.investing.com' + \\\n",
    "            item.select_one(\".title\").get(\"href\")\n",
    "        if date != []:\n",
    "            df.append({'symbol': symbol, 'title': title,\n",
    "                       \"link\": link, 'date': date})\n",
    "    # time.sleep(0.5)\n",
    "\n",
    "    if today >= update_date:\n",
    "        d = crawling_investing(symbol, query)\n",
    "        crawling_investings.append(d)\n",
    "        querys['update_date'][querys['Symbol'] == symbol] = d[-1]\n",
    "        crawling_investings2.append(crawling_investing2(symbol, query))\n",
    "\n",
    "\n",
    "try:\n",
    "    crawling_investings[0]\n",
    "\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    crawling_investings.to_csv(\n",
    "        '../datas/crawling_investing.csv', index=False, header=False, mode='a')\n",
    "    print('update crawling_investing')\n",
    "    a = 1\n",
    "\n",
    "\n",
    "try:\n",
    "    crawling_investings2[0]\n",
    "    datas = crawling_investings2\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    crawling_investings2 = pd.DataFrame(datas)\n",
    "    crawling_investings2.to_csv(\n",
    "        '../datas/crawling_investing2.csv', index=False, header=False, mode='a')\n",
    "    print('update crawling_investing2')\n",
    "    a = 1\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv('../datas/investing_news.csv', index=False, header=False, mode='a')\n",
    "\n",
    "if a == 1:\n",
    "    querys.to_csv('../datas/investing_query.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
